{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_query: (1497, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>related_image</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>请对给定的图片进行描述。</td>\n",
       "      <td>vwsscflkvakdictzacfx.jpg</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>这款运动裤是什么材质做的？</td>\n",
       "      <td>jjxjzgkbrfizjwfngwis.jpg</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        question             related_image answer\n",
       "0   请对给定的图片进行描述。  vwsscflkvakdictzacfx.jpg       \n",
       "1  这款运动裤是什么材质做的？  jjxjzgkbrfizjwfngwis.jpg       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_train: (12768, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scqxwrymypdzdefummyj.jpg</td>\n",
       "      <td>无拘2019女夏新款衬衫裙夏装格纹收腰气质显瘦蕾丝腰带衬衫连衣裙</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chvgdtmndrqwfkabrgoh.jpg</td>\n",
       "      <td>2019夏季新款高端气质不对称肩带chic修身显瘦日常V领连衣裙女潮</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      image                                text\n",
       "0  scqxwrymypdzdefummyj.jpg    无拘2019女夏新款衬衫裙夏装格纹收腰气质显瘦蕾丝腰带衬衫连衣裙\n",
       "1  chvgdtmndrqwfkabrgoh.jpg  2019夏季新款高端气质不对称肩带chic修身显瘦日常V领连衣裙女潮"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import json \n",
    "\n",
    "\n",
    "with open(\"./data/query.json\", \"r\") as f:\n",
    "    query = json.load(f)\n",
    "    df_query = pd.DataFrame(query)\n",
    "df_train = pd.read_csv(\"./data/train_annotation.csv\", sep=\"\\t\")\n",
    "print(f\"Shape of df_query: {df_query.shape}\")\n",
    "display(df_query.head(2))\n",
    "print(f\"Shape of df_train: {df_train.shape}\")\n",
    "display(df_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 22:54:20.959074: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-31 22:54:20.986268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-31 22:54:21.014974: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-31 22:54:21.023744: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-31 22:54:21.050380: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-31 22:54:22.482117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import ChineseCLIPProcessor, ChineseCLIPModel\n",
    "import os\n",
    "import torch \n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 设置镜像端点\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"hf-mirror\"\n",
    "clip_model_path = \"/root/onethingai-tmp/models--OFA-Sys--chinese-clip-vit-huge-patch14/snapshots/503e16b560aff94c1922f13a86a7693d36957a4f\"\n",
    "model = ChineseCLIPModel.from_pretrained(clip_model_path).to(device)\n",
    "processor = ChineseCLIPProcessor.from_pretrained(clip_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "import torch \n",
    "import transformers\n",
    "\n",
    "\n",
    "# 只微调qkv\n",
    "target_modules = []\n",
    "for i in range(24):\n",
    "    target_modules.append(f\"text_model.encoder.layer.{i}.attention.self.query\")\n",
    "    target_modules.append(f\"text_model.encoder.layer.{i}.attention.self.key\")\n",
    "    target_modules.append(f\"text_model.encoder.layer.{i}.attention.self.value\")\n",
    "    \n",
    "for i in range(32):\n",
    "    target_modules.append(f\"vision_model.emcoder.layers.{i}.self_attn.k_proj\")\n",
    "    target_modules.append(f\"vision_model.emcoder.layers.{i}.self_attn.v_proj\")\n",
    "    target_modules.append(f\"vision_model.emcoder.layers.{i}.self_attn.q_proj\")\n",
    "\n",
    "# LoRA配置\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=96,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "# 将 LoRA 应用于模型\n",
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12768, 3)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "import numpy as np \n",
    "\n",
    "df_train = pd.read_csv(\"./data/train_annotation.csv\", sep=\"\\t\")\n",
    "# df_train, df_valid = train_test_split(df_train, test_size=1250, random_state=42)\n",
    "img_path = \"./data/image\"\n",
    "df_train[\"image_path\"] = df_train[\"image\"].apply(lambda x: os.path.join(img_path, x))\n",
    "# df_valid[\"image_path\"] = df_valid[\"image\"].apply(lambda x: os.path.join(img_path, x))\n",
    "# print(df_train.shape, df_valid.shape)\n",
    "print(df_train.shape)\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "# valid_dataset = Dataset.from_pandas(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 22:54:24,856 [INFO] Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding', 'truncation', 'max_length'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "2024-08-31 23:02:25,122 [INFO] Training Loss: 0.8639091522920699, Acc: 0.7596334586466166\n",
      "2024-08-31 23:02:25,251 [INFO] Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding', 'truncation', 'max_length'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "2024-08-31 23:10:24,095 [INFO] Training Loss: 0.7898721013750348, Acc: 0.7761591478696742\n",
      "2024-08-31 23:10:24,214 [INFO] Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding', 'truncation', 'max_length'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "2024-08-31 23:18:20,991 [INFO] Training Loss: 0.7430265659377688, Acc: 0.7817982456140351\n",
      "2024-08-31 23:18:21,105 [INFO] Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding', 'truncation', 'max_length'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "2024-08-31 23:26:18,980 [INFO] Training Loss: 0.7115312275432405, Acc: 0.7945645363408521\n",
      "2024-08-31 23:26:19,104 [INFO] Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding', 'truncation', 'max_length'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "2024-08-31 23:34:15,632 [INFO] Training Loss: 0.6880423426628113, Acc: 0.7950344611528822\n",
      "2024-08-31 23:34:15,768 [INFO] Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding', 'truncation', 'max_length'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "2024-08-31 23:42:14,506 [INFO] Training Loss: 0.6610280615942818, Acc: 0.8046679197994987\n",
      "2024-08-31 23:42:14,628 [INFO] Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding', 'truncation', 'max_length'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "2024-08-31 23:50:11,960 [INFO] Training Loss: 0.6421245989345369, Acc: 0.8049812030075187\n",
      "2024-08-31 23:50:12,076 [INFO] Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding', 'truncation', 'max_length'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "2024-08-31 23:58:11,826 [INFO] Training Loss: 0.6175007195699782, Acc: 0.8122650375939849\n",
      "2024-08-31 23:58:11,941 [INFO] Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding', 'truncation', 'max_length'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "2024-09-01 00:06:10,692 [INFO] Training Loss: 0.596953272819519, Acc: 0.8176691729323309\n",
      "2024-09-01 00:06:10,813 [INFO] Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding', 'truncation', 'max_length'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "2024-09-01 00:14:10,262 [INFO] Training Loss: 0.5897957909674871, Acc: 0.818609022556391\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "\n",
    "# 配置 logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"train_all.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=4e-5)\n",
    "# 定义批量\n",
    "batch_size = 625\n",
    "# 定义dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "# 训练模型的自定义循环\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False, dynamic_ncols=True)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # 将数据移动到GPU\n",
    "        images = []\n",
    "        for image_path in batch[\"image_path\"]:\n",
    "            try:\n",
    "                images.append(Image.open(image_path).transpose(Image.FLIP_LEFT_RIGHT).convert(\"RGB\"))\n",
    "            except:\n",
    "                images.append(Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8)).convert(\"RGB\"))\n",
    "        inputs = processor(text=batch[\"text\"], images=images, return_tensors=\"pt\", padding=True, truncation=True, max_length=52).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs[\"logits_per_image\"]\n",
    "        logits_per_text = outputs[\"logits_per_text\"]\n",
    "        labels = torch.arange(logits_per_image.size(0), device=device)\n",
    "        # 计算损失\n",
    "        loss_text = F.cross_entropy(logits_per_text, labels)\n",
    "        loss_image = F.cross_entropy(logits_per_image, labels)\n",
    "        loss = (loss_text + loss_image) / 2\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # 保存预测和标签\n",
    "        preds = torch.argmax(logits_per_text, dim=1).detach().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, acc\n",
    "\n",
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False, dynamic_ncols=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # 将数据移动到GPU\n",
    "            images = []\n",
    "            for image_path in batch[\"image_path\"]:\n",
    "                try:\n",
    "                    images.append(Image.open(image_path).convert(\"RGB\"))\n",
    "                except:\n",
    "                    images.append(Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8)).transpose(Image.FLIP_LEFT_RIGHT).convert(\"RGB\"))\n",
    "            inputs = processor(text=batch[\"text\"], images=images, return_tensors=\"pt\", padding=True, truncation=True, max_length=52).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs[\"logits_per_image\"]\n",
    "            logits_per_text = outputs[\"logits_per_text\"]\n",
    "            labels = torch.arange(logits_per_image.size(0), device=device)\n",
    "            # 计算损失\n",
    "            loss_text = F.cross_entropy(logits_per_text, labels)\n",
    "            loss_image = F.cross_entropy(logits_per_image, labels)\n",
    "            loss = (loss_text + loss_image) / 2\n",
    "            total_loss += loss.item()\n",
    "            # 保存预测和标签\n",
    "            preds = torch.argmax(logits_per_text, dim=1).detach().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, acc\n",
    "\n",
    "# 设置Epoch\n",
    "num_epochs = 10\n",
    "# 设置设备为GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lora_model.to(device)\n",
    "# logging.info(f\"Epoch 0/{num_epochs}\")\n",
    "# valid_loss, valid_acc = evaluate_epoch(lora_model, valid_dataloader, device)\n",
    "# logging.info(f\"Eval Loss: {valid_loss}, Acc: {valid_acc}\")\n",
    "# train_loss, train_acc = evaluate_epoch(lora_model, train_dataloader, device)\n",
    "# logging.info(f\"Eval Loss: {train_loss}, Acc: {train_acc}\")\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    logging.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    # 训练一个epoch\n",
    "    train_loss, train_acc = train_epoch(lora_model, train_dataloader, optimizer, device)\n",
    "    logging.info(f\"Training Loss: {train_loss}, Acc: {train_acc}\")\n",
    "    # # 在验证集上评估\n",
    "    # valid_loss, valid_acc = evaluate_epoch(lora_model, valid_dataloader, device)\n",
    "    # logging.info(f\"Validation Loss: {valid_loss}, Acc: {valid_acc}\")\n",
    "    lora_model.save_pretrained(f\"./adapter/adapter_{epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lora_model, \"CLIP_LoRA_625_10.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
